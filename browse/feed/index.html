<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="https://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="https://wellformedweb.org/CommentAPI/"
	xmlns:dc="https://purl.org/dc/elements/1.1/"
	xmlns:atom="https://www.w3.org/2005/Atom"
	xmlns:sy="https://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="https://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Products &#8211; Angelos Barmpoutis</title>
	<atom:link href="https://abarmpou.github.io/angelos/browse/feed/" rel="self" type="application/rss+xml" />
	<link>https://abarmpou.github.io/angelos</link>
	<description>Professor of Digital Arts and Sciences</description>
	<lastBuildDate>Thu, 15 Feb 2024 21:53:25 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.3</generator>
	<item>
		<title>Automated analysis of human motion using AI</title>
		<link>https://abarmpou.github.io/angelos/page/automated-analysis-of-human-motion-using-ai/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Thu, 15 Feb 2024 03:59:48 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=306</guid>

					<description><![CDATA[In this project we use deep learning algorithms to design fully automated methods for analyzing human motion and classifying it into distinct patterns. One application of the methods developed in this project is automated choreographic analysis by identifying the distinct dance move patterns in specific genres such as ballet, Afrogenic dance, etc. Our results have been presented in a series of papers on this topic.]]></description>
										<content:encoded><![CDATA[<p>Human movement has been studied in multiple disciplines, including health sciences (biomechanics, kinesiology, neurology, sports medicine) and the Arts (theater and dance, cultural studies) as well as their intersection (arts in medicine, dance therapy), resulting in a large but disparate assortment of multi-modal datasets, including video, skeletal motion capture, manual annotations, and clinical metadata. </p>
<p>Traditional data collection processes often include Laban movement analysis, a standardized form of human movement annotation that parameterizes the observed motion changes in a pre-defined 4-dimensional feature space (effort, space, shape, body timing/phrasing). Such analysis requires lengthy manual input from professionals who annotate the recorded data through a time-consuming &#8220;watch and pause&#8221; process, which is also prone to human errors.</p>
<p>In this project, we use AI to fully automate the annotation process involved in Laban analysis by training a Bayesian-layered (in the temporal domain) Convolutional Neural Network (CNN) through supervised learning on existing human motion datasets of video and skeletal sequences.</p>
<p>The trained model is beeing tested in Laban-annotating existing video and skeletal sequences and validated by arts in medicine practitioners and experts in Laban analysis. This AI-driven project will have a significant impact as it will enable automated classification and understanding of human motion across a spectrum of movement-centered disciplines, including clinical and telehealth settings, orthopedic centers, choreographic practice, and cross-cultural movement analysis.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Passive Haptics in Virtual Reality</title>
		<link>https://abarmpou.github.io/angelos/page/passive-haptics-in-virtual-reality/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Thu, 15 Feb 2024 03:28:56 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=304</guid>

					<description><![CDATA[We have been running various user studies at UF Reality Lab that investigate the role of tangible objects in virtual reality. A series of publications co-authored by our graduate students in the Digital Arts &#038; Sciences program have been assessing the influence of low-cost passive haptic interfaces on the user's perception and overall experience within virtual environments.]]></description>
										<content:encoded><![CDATA[<p>We have been running various user studies at UF Reality Lab that investigate the role of tangible objects in virtual reality. A series of publications co-authored by our graduate students in the Digital Arts &amp; Sciences program have been assessing the influence of low-cost passive haptic interfaces on the user&#8217;s perception and overall experience within virtual environments.</p>
<p><strong>How do passive haptics affect the user&#8217;s perception of physical properties, such as weight and size, in Virtual Reality? (HCII 2024)</p>
<p>How does interaction with physical objects within virtual environments affect knowledge acquisition and recall? (HCII 2024)</p>
<p>How do low-cost passive haptics affect the user experience while exercising? (HCII 2020)</strong></p>
<p><iframe src="https://www.youtube.com/embed/QiO9ZzyffAY?feature=oembed" width="1200" height="600" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Assessing the Influence of Passive Haptics on User Perception of Physical Properties in Virtual Reality</title>
		<link>https://abarmpou.github.io/angelos/page/assessing-the-influence-of-passive-haptics-on-user-perception-of-physical-properties-in-virtual-reality/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Wed, 14 Feb 2024 18:49:54 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=298</guid>

					<description><![CDATA[This paper presents a pilot study that explores the role of low-cost passive haptics on how users perceive physical properties such as the size and weight of objects within virtual reality environments. An A/B-type study was conducted as an air hockey simulation in which participants experienced two versions: one adhered to conventional VR settings, while<span class="read-more-faq"><a href="https://abarmpou.github.io/angelos/page/assessing-the-influence-of-passive-haptics-on-user-perception-of-physical-properties-in-virtual-reality/">Read More</a></span>]]></description>
										<content:encoded><![CDATA[<p>This paper presents a pilot study that explores the role of low-cost passive haptics on how users perceive physical properties such as the size and weight of objects within virtual reality environments. An A/B-type study was conducted as an air hockey simulation in which<br />
participants experienced two versions: one adhered to conventional VR settings, while the other incorporated a tangible surface, a real table. Statistical analysis of the data collected from post-study questionnaires indicated a shift in perception of size and weight when exposed to the haptic-enhanced simulation, with virtual objects perceived as larger or heavier. It was also noted that the observed shift of the user perception was stronger when the simulation with the tangible surface was experienced first. The paper presents details on the implementation of the air hockey simulation and the setup within the testing environment as well as the statistical analysis performed on the collected data, offering practical recommendations for future applications.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Java for Kinect (J4K)</title>
		<link>https://abarmpou.github.io/angelos/page/java-for-kinect-j4k-2/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Sat, 13 Jan 2024 22:22:13 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=284</guid>

					<description><![CDATA[]]></description>
										<content:encoded><![CDATA[]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Java for Oculus Quest (J4Q)</title>
		<link>https://abarmpou.github.io/angelos/page/java-for-oculus-quest-j4q/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Sat, 13 Jan 2024 22:21:20 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=282</guid>

					<description><![CDATA[]]></description>
										<content:encoded><![CDATA[]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Android for Beginners</title>
		<link>https://abarmpou.github.io/angelos/page/android-for-beginners/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Sat, 13 Jan 2024 22:20:20 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=280</guid>

					<description><![CDATA[]]></description>
										<content:encoded><![CDATA[]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Reinscribing the 3rd dimension in epigraphic studies and transcending disciplinary boundaries</title>
		<link>https://abarmpou.github.io/angelos/page/reinscribing-the-3rd-dimension-in-epigraphic-studies-and-transcending-disciplinary-boundaries/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Sun, 31 Dec 2023 20:26:10 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=300</guid>

					<description><![CDATA[Over the past decade, archaeology and epigraphy have been reconsidering their modus operandi. Prompted and facilitated by technological advances, motivated by new research questions, and challenged by growing calls to engage with contemporary audiences, they have been experimenting with methodological approaches and interdisciplinary collaborations. Within this context, the Digital Epigraphy and Archaeology project (DEA) has<span class="read-more-faq"><a href="https://abarmpou.github.io/angelos/page/reinscribing-the-3rd-dimension-in-epigraphic-studies-and-transcending-disciplinary-boundaries/">Read More</a></span>]]></description>
										<content:encoded><![CDATA[<p>Over the past decade, archaeology and epigraphy have been reconsidering their modus operandi. Prompted and facilitated by technological advances, motivated by new research questions, and challenged by growing calls to engage with contemporary audiences, they have been experimenting with methodological approaches and interdisciplinary collaborations. Within this context, the Digital Epigraphy and Archaeology project (DEA) has been developing 3D digitization techniques that accommodate various types of artifacts, has been incorporating multidisciplinary approaches to achieve a more holistic stance towards the objects of study, and has focused on the reproducibility and accessibility of both its techniques and the 3D models.</p>
<p>This paper presents the DEA’s introspective and reembodied ways of preserving and studying the past by reconsidering historical artifacts and their digital re-materialization. The following sections discuss the project’s approach to copies and digital copies, 3D digitization and enhanced visualization processes, comprehensive cloud services, and 3D printing to present the DEA steps toward facilitating and advancing archaeology and epigraphy. Through such approaches that combine traditional rigor with technological novelty and affordances, the team’s vision is to popularize archaeology and epigraphy within and beyond academia and pinpoint the significance of the world’s heritage to the new generations of students and the public.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Prostate Capsule Segmentation in Micro-Ultrasound Images Using Deep Neural Networks</title>
		<link>https://abarmpou.github.io/angelos/page/prostate-capsule-segmentation-in-micro-ultrasound-images-using-deep-neural-networks/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Tue, 18 Apr 2023 23:36:13 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=287</guid>

					<description><![CDATA[Prostate cancer is the most common internal malignancy among males. Micro-Ultrasound is a promising imaging modality for cancer identification and computer-assisted visualization. Identifying the prostate capsule area is essential in active surveillance monitoring and treatment planning. In this paper, we present a pilot study that assesses prostate capsule segmentation using the U-Net deep neural network<span class="read-more-faq"><a href="https://abarmpou.github.io/angelos/page/prostate-capsule-segmentation-in-micro-ultrasound-images-using-deep-neural-networks/">Read More</a></span>]]></description>
										<content:encoded><![CDATA[<p>Prostate cancer is the most common internal malignancy among males. Micro-Ultrasound is a promising imaging modality for cancer identification and computer-assisted visualization. Identifying the prostate capsule area is essential in active surveillance monitoring and treatment planning. In this paper, we present a pilot study that assesses prostate capsule segmentation using the U-Net deep neural network framework. To the best of our knowledge, this is the first study on prostate capsule segmentation in Micro-Ultrasound images. For our study, we collected multi-frame volumes of Micro-Ultrasound images, and then expert prostate cancer surgeons annotated the capsule border manually. The lack of clear boundaries and variation of shapes between patients make the task challenging, especially for novice Micro-Ultrasound operators. In total 2099 images were collected from 8 subjects, 1296 of which were manually annotated and were split into a training set (1008), a validation set (112), and a test set from a different subject (176). The performance of the model was evaluated by calculating the Intersection over Union (IoU) between the manually annotated area of the capsule and the segmentation mask computed from the trained deep neural network. The results demonstrate high IoU values for the training set (95.05%), the validation set (93.18%) and the test set from a separate subject (85.14%). In 10-fold cross-validation, IoU was 94.25%, and accuracy was 99%, validating the robustness of the model. Our pilot study demonstrates that deep neural networks can produce reliable segmentation of the prostate capsule in Micro-Ultrasound images and pave the road for the segmentation of other anatomical structures within the capsule, which will be the subject of our future studies.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Developing Mini VR Game Engines as an Engaging Learning Method for Digital Arts &#038; Sciences</title>
		<link>https://abarmpou.github.io/angelos/page/developing-mini-vr-game-engines-as-an-engaging-learning-method-for-digital-arts-sciences/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Sat, 11 Mar 2023 23:42:04 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=289</guid>

					<description><![CDATA[Digital Arts and Sciences curricula have been known for combining topics of emerging technologies and artistic creativity for the professional preparation of future technical artists and other creative media professionals. One of the key challenges in such an interdisciplinary curriculum is the instruction of complex technical concepts to an audience that lacks prior computer science<span class="read-more-faq"><a href="https://abarmpou.github.io/angelos/page/developing-mini-vr-game-engines-as-an-engaging-learning-method-for-digital-arts-sciences/">Read More</a></span>]]></description>
										<content:encoded><![CDATA[<p>Digital Arts and Sciences curricula have been known for combining topics of emerging technologies and artistic creativity for the professional preparation of future technical artists and other creative media professionals. One of the key challenges in such an interdisciplinary curriculum is the instruction of complex technical concepts to an audience that lacks prior computer science background. This paper discusses how developing small custom virtual and augmented reality game engines can become an effective and engaging method for teaching various fundamental technical topics from Digital Arts and Sciences curricula. Based on empirical evidence, we demonstrate examples that integrate concepts from geometry, linear algebra, and computer programming to 3D modeling, animation, and procedural art. The paper also introduces an open-source framework for implementing such a curriculum in Quest VR headsets, and we provide examples of small-scale focused exercises and learning activities.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI-driven Human Motion Classification and Analysis using Laban Movement System</title>
		<link>https://abarmpou.github.io/angelos/page/ai-driven-human-motion-classification-and-analysis-using-laban-movement-system/</link>
		
		<dc:creator><![CDATA[angelos]]></dc:creator>
		<pubDate>Thu, 14 Jul 2022 18:30:46 +0000</pubDate>
				<guid isPermaLink="false">https://abarmpou.github.io/angelos/?post_type=product&#038;p=295</guid>

					<description><![CDATA[Human movement classification and analysis are important in the research of health sciences and the arts. Laban movement analysis is an effective method to annotate human movement in dance that describes communication and expression. Technology-supported human movement analysis employs motion sensors, infrared cameras, and other wearable devices to capture critical joints of the human skeleton<span class="read-more-faq"><a href="https://abarmpou.github.io/angelos/page/ai-driven-human-motion-classification-and-analysis-using-laban-movement-system/">Read More</a></span>]]></description>
										<content:encoded><![CDATA[<p>Human movement classification and analysis are important in the research of health sciences and the arts. Laban movement analysis is an effective method to annotate human movement in dance that describes communication and expression. Technology-supported human movement analysis employs motion sensors, infrared cameras, and other wearable devices to capture critical joints of the human skeleton and facial key points. However, the aforementioned technologies are not mainstream, and the most popular form of motion capture is conventional video recording, usually from a single stationary camera. Such video recordings can be used to evaluate human movement or dance performance. Any methods that can systematically analyze and annotate these raw video footage would be of great importance to this field. Therefore, this research offers an analysis and comparison of AI-based computer vision methods that can annotate the human movement automatically. This study trained and compared four different machine learning algorithms (random forest, K neighbors, neural network, and decision tree) through supervised learning on existing video datasets of dance performances. The developed system was able to automatically produce annotation in the four dimensions (effort, space, shape, body) of Laban movement analysis. The results demonstrate accurately produced annotations in comparison to manually entered ground truth Laban annotation.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
